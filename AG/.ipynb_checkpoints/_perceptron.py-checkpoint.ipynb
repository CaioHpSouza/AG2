{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Mathieu Blondel\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from ..utils.validation import _deprecate_positional_args\n",
    "from ._stochastic_gradient import BaseSGDClassifier\n",
    "\n",
    "\n",
    "class Perceptron(BaseSGDClassifier):\n",
    "    \"\"\"Perceptron\n",
    "\n",
    "    Read more in the :ref:`User Guide <perceptron>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    penalty : {'l2','l1','elasticnet'}, default=None\n",
    "        The penalty (aka regularization term) to be used.\n",
    "\n",
    "    alpha : float, default=0.0001\n",
    "        Constant that multiplies the regularization term if regularization is\n",
    "        used.\n",
    "\n",
    "    l1_ratio : float, default=0.15\n",
    "        The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\n",
    "        `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\n",
    "        Only used if `penalty='elasticnet'`.\n",
    "\n",
    "        .. versionadded:: 0.24\n",
    "\n",
    "    fit_intercept : bool, default=True\n",
    "        Whether the intercept should be estimated or not. If False, the\n",
    "        data is assumed to be already centered.\n",
    "\n",
    "    max_iter : int, default=1000\n",
    "        The maximum number of passes over the training data (aka epochs).\n",
    "        It only impacts the behavior in the ``fit`` method, and not the\n",
    "        :meth:`partial_fit` method.\n",
    "\n",
    "        .. versionadded:: 0.19\n",
    "\n",
    "    tol : float, default=1e-3\n",
    "        The stopping criterion. If it is not None, the iterations will stop\n",
    "        when (loss > previous_loss - tol).\n",
    "\n",
    "        .. versionadded:: 0.19\n",
    "\n",
    "    shuffle : bool, default=True\n",
    "        Whether or not the training data should be shuffled after each epoch.\n",
    "\n",
    "    verbose : int, default=0\n",
    "        The verbosity level\n",
    "\n",
    "    eta0 : double, default=1\n",
    "        Constant by which the updates are multiplied.\n",
    "\n",
    "    n_jobs : int, default=None\n",
    "        The number of CPUs to use to do the OVA (One Versus All, for\n",
    "        multi-class problems) computation.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    random_state : int, RandomState instance, default=None\n",
    "        Used to shuffle the training data, when ``shuffle`` is set to\n",
    "        ``True``. Pass an int for reproducible output across multiple\n",
    "        function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "\n",
    "    early_stopping : bool, default=False\n",
    "        Whether to use early stopping to terminate training when validation.\n",
    "        score is not improving. If set to True, it will automatically set aside\n",
    "        a stratified fraction of training data as validation and terminate\n",
    "        training when validation score is not improving by at least tol for\n",
    "        n_iter_no_change consecutive epochs.\n",
    "\n",
    "        .. versionadded:: 0.20\n",
    "\n",
    "    validation_fraction : float, default=0.1\n",
    "        The proportion of training data to set aside as validation set for\n",
    "        early stopping. Must be between 0 and 1.\n",
    "        Only used if early_stopping is True.\n",
    "\n",
    "        .. versionadded:: 0.20\n",
    "\n",
    "    n_iter_no_change : int, default=5\n",
    "        Number of iterations with no improvement to wait before early stopping.\n",
    "\n",
    "        .. versionadded:: 0.20\n",
    "\n",
    "    class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
    "        Preset for the class_weight fit parameter.\n",
    "\n",
    "        Weights associated with classes. If not given, all classes\n",
    "        are supposed to have weight one.\n",
    "\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data\n",
    "        as ``n_samples / (n_classes * np.bincount(y))``\n",
    "\n",
    "    warm_start : bool, default=False\n",
    "        When set to True, reuse the solution of the previous call to fit as\n",
    "        initialization, otherwise, just erase the previous solution. See\n",
    "        :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray of shape (n_classes,)\n",
    "        The unique classes labels.\n",
    "\n",
    "    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else \\\n",
    "            (n_classes, n_features)\n",
    "        Weights assigned to the features.\n",
    "\n",
    "    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
    "        Constants in decision function.\n",
    "\n",
    "    loss_function_ : concrete LossFunction\n",
    "        The function that determines the loss, or difference between the\n",
    "        output of the algorithm and the target values.\n",
    "\n",
    "    n_iter_ : int\n",
    "        The actual number of iterations to reach the stopping criterion.\n",
    "        For multiclass fits, it is the maximum over every binary fit.\n",
    "\n",
    "    t_ : int\n",
    "        Number of weight updates performed during training.\n",
    "        Same as ``(n_iter_ * n_samples)``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    ``Perceptron`` is a classification algorithm which shares the same\n",
    "    underlying implementation with ``SGDClassifier``. In fact,\n",
    "    ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n",
    "    eta0=1, learning_rate=\"constant\", penalty=None)`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_digits\n",
    "    >>> from sklearn.linear_model import Perceptron\n",
    "    >>> X, y = load_digits(return_X_y=True)\n",
    "    >>> clf = Perceptron(tol=1e-3, random_state=0)\n",
    "    >>> clf.fit(X, y)\n",
    "    Perceptron()\n",
    "    >>> clf.score(X, y)\n",
    "    0.939...\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SGDClassifier\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    https://en.wikipedia.org/wiki/Perceptron and references therein.\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, *, penalty=None, alpha=0.0001, l1_ratio=0.15,\n",
    "                 fit_intercept=True,\n",
    "                 max_iter=1000, tol=1e-3, shuffle=True, verbose=0, eta0=1.0,\n",
    "                 n_jobs=None, random_state=0, early_stopping=False,\n",
    "                 validation_fraction=0.1, n_iter_no_change=5,\n",
    "                 class_weight=None, warm_start=False):\n",
    "        super().__init__(\n",
    "            loss=\"perceptron\", penalty=penalty, alpha=alpha, l1_ratio=l1_ratio,\n",
    "            fit_intercept=fit_intercept, max_iter=max_iter, tol=tol,\n",
    "            shuffle=shuffle, verbose=verbose, random_state=random_state,\n",
    "            learning_rate=\"constant\", eta0=eta0, early_stopping=early_stopping,\n",
    "            validation_fraction=validation_fraction,\n",
    "            n_iter_no_change=n_iter_no_change, power_t=0.5,\n",
    "            warm_start=warm_start, class_weight=class_weight, n_jobs=n_jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
